{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling with saved models\n",
    "\n",
    "You can modify the file at \"./save/checkpoint\". The first line:\n",
    "\n",
    "    model_checkpoint_path: \"model.ckpt-???\"\n",
    "    \n",
    "indicates the saved model you will use for sampling. The default value will be the last model you've saved.\n",
    "\n",
    "Run the following two code block for sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib import legacy_seq2seq\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, args, training=True):\n",
    "        self.args = args\n",
    "        if not training:\n",
    "            args.batch_size = 1\n",
    "            args.seq_length = 1\n",
    "\n",
    "        # choose different rnn cell \n",
    "        if args.model == 'rnn':\n",
    "            cell_fn = rnn.RNNCell\n",
    "        elif args.model == 'gru':\n",
    "            cell_fn = rnn.GRUCell\n",
    "        elif args.model == 'lstm':\n",
    "            cell_fn = rnn.LSTMCell\n",
    "        elif args.model == 'nas':\n",
    "            cell_fn = rnn.NASCell\n",
    "        else:\n",
    "            raise Exception(\"model type not supported: {}\".format(args.model))\n",
    "\n",
    "        # warp multi layered rnn cell into one cell with dropout\n",
    "        cells = []\n",
    "        for _ in range(args.num_layers):\n",
    "            cell = cell_fn(args.rnn_size)\n",
    "            if training and (args.output_keep_prob < 1.0 or args.input_keep_prob < 1.0):\n",
    "                cell = rnn.DropoutWrapper(cell,\n",
    "                                          input_keep_prob=args.input_keep_prob,\n",
    "                                          output_keep_prob=args.output_keep_prob)\n",
    "            cells.append(cell)\n",
    "        self.cell = cell = rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "\n",
    "        # input/target data (int32 since input is char-level)\n",
    "        self.input_data = tf.placeholder(\n",
    "            tf.int32, [args.batch_size, args.seq_length])\n",
    "        self.targets = tf.placeholder(\n",
    "            tf.int32, [args.batch_size, args.seq_length])\n",
    "        self.initial_state = cell.zero_state(args.batch_size, tf.float32)\n",
    "\n",
    "        # softmax output layer, use softmax to classify\n",
    "        with tf.variable_scope('rnnlm'):\n",
    "            softmax_w = tf.get_variable(\"softmax_w\", [args.rnn_size, args.vocab_size])\n",
    "            softmax_b = tf.get_variable(\"softmax_b\", [args.vocab_size])\n",
    "\n",
    "        # transform input to embedding\n",
    "        embedding = tf.get_variable(\"embedding\", [args.vocab_size, args.rnn_size])\n",
    "        inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "\n",
    "        # dropout beta testing: double check which one should affect next line\n",
    "        if training and args.output_keep_prob:\n",
    "            inputs = tf.nn.dropout(inputs, args.output_keep_prob)\n",
    "\n",
    "        # unstack the input to fits in rnn model\n",
    "        inputs = tf.split(inputs, args.seq_length, 1)\n",
    "        inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "\n",
    "        # loop function for rnn_decoder, which take the previous i-th cell's output and generate the (i+1)-th cell's input\n",
    "        def loop(prev, _):\n",
    "            prev = tf.matmul(prev, softmax_w) + softmax_b\n",
    "            prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "            return tf.nn.embedding_lookup(embedding, prev_symbol)\n",
    "\n",
    "        # rnn_decoder to generate the ouputs and final state. When we are not training the model, we use the loop function.\n",
    "        outputs, last_state = legacy_seq2seq.rnn_decoder(inputs, self.initial_state, cell, loop_function=loop if not training else None, scope='rnnlm')\n",
    "        output = tf.reshape(tf.concat(outputs, 1), [-1, args.rnn_size])\n",
    "\n",
    "        # output layer\n",
    "        self.logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        self.probs = tf.nn.softmax(self.logits)\n",
    "\n",
    "        # loss is calculate by the log loss and taking the average.\n",
    "        loss = legacy_seq2seq.sequence_loss_by_example(\n",
    "                [self.logits],\n",
    "                [tf.reshape(self.targets, [-1])],\n",
    "                [tf.ones([args.batch_size * args.seq_length])])\n",
    "        with tf.name_scope('cost'):\n",
    "            self.cost = tf.reduce_sum(loss) / args.batch_size / args.seq_length\n",
    "        self.final_state = last_state\n",
    "        self.lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "\n",
    "        # calculate gradients\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),\n",
    "                args.grad_clip)\n",
    "        with tf.name_scope('optimizer'):\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "\n",
    "        # apply gradient change to the all the trainable variable.\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "        # instrument tensorboard\n",
    "        tf.summary.histogram('logits', self.logits)\n",
    "        tf.summary.histogram('loss', loss)\n",
    "        tf.summary.scalar('train_loss', self.cost)\n",
    "\n",
    "    def sample(self, sess, chars, vocab, num=200, prime='The ', sampling_type=1):\n",
    "        state = sess.run(self.cell.zero_state(1, tf.float32))\n",
    "        for char in prime[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state: state}\n",
    "            [state] = sess.run([self.final_state], feed)\n",
    "\n",
    "        def weighted_pick(weights):\n",
    "            t = np.cumsum(weights)\n",
    "            s = np.sum(weights)\n",
    "            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "        ret = prime\n",
    "        char = prime[-1]\n",
    "        for _ in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state: state}\n",
    "            [probs, state] = sess.run([self.probs, self.final_state], feed)\n",
    "            p = probs[0]\n",
    "\n",
    "            if sampling_type == 0:\n",
    "                sample = np.argmax(p)\n",
    "            elif sampling_type == 2:\n",
    "                if char == ' ':\n",
    "                    sample = weighted_pick(p)\n",
    "                else:\n",
    "                    sample = np.argmax(p)\n",
    "            else:  # sampling_type == 1 default:\n",
    "                sample = weighted_pick(p)\n",
    "\n",
    "            pred = chars[sample]\n",
    "            ret += pred\n",
    "            char = pred\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/model.ckpt-50\n",
      "b\" casdebiu st itty oa nlorwloeAw:lr\\ne tew?OMe:ystKdlh aeu\\nrarriTugprwlgercs u aoIK: IS hEt!w  nnh h; a imyerddot:  niia tr\\nielTav peeiyahahf-d'e,E hvgtt b  ct?rwoi iSyCwt uwm\\nfiwdg c\\nankke fhd oH myvh Tso e:v. oaReeberdeI  orwk a nmeah,  ydmtreeerohoooeys ohcUib\\n:o nbdsOwaanld dbc mie Tk\\n. btieiRehDnetHIws nbopigi,e MCta,V  IiC\\naLfg g:Tnuhny e tuf shudu' yi a?ntoe hWnIO b, ttittU u,hgyem Gt hak rMIKh gtr aoneh\\n nnrg\\nU?ssneiveMg h\\nvab oulobioPatiaoTrnn,ko o tUtbnAec hkadedor c ,h uolhy \\n Iueraowint\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from six.moves import cPickle\n",
    "from six import text_type\n",
    "\n",
    "tf.flags.DEFINE_string('data_dir', 'data/tinyshakespeare', 'data directory containing input.txt with training examples')\n",
    "tf.flags.DEFINE_string('save_dir', 'save', 'directory to store checkpointed models')\n",
    "tf.flags.DEFINE_string('log_dir', 'logs', 'directory to store tensorboard logs')\n",
    "tf.flags.DEFINE_integer('save_every', 1000, 'Save frequency. Number of passes between checkpoints of the model.')\n",
    "tf.flags.DEFINE_string('init_from', None, \"continue training from saved model at this path.\")\n",
    "\n",
    "# Model params\n",
    "\n",
    "tf.flags.DEFINE_string('model', 'lstm', 'lstm, rnn, gru, or nas')\n",
    "tf.flags.DEFINE_integer('rnn_size', 128, 'size of RNN hidden state')\n",
    "tf.flags.DEFINE_integer('num_layers', 2, 'number of layers in the RNN')\n",
    "\n",
    "# Optimization\n",
    "\n",
    "tf.flags.DEFINE_integer('seq_length', 50, 'RNN sequence length. Number of timesteps to unroll for.')\n",
    "tf.flags.DEFINE_integer('batch_size', 50, 'minibatch size.')\n",
    "tf.flags.DEFINE_integer('num_epochs', 1, 'number of epochs. Number of full passes through the training examples.')\n",
    "tf.flags.DEFINE_float('grad_clip', 5., 'clip gradients at this value')\n",
    "tf.flags.DEFINE_float('learning_rate', 0.002, 'learning rate')\n",
    "tf.flags.DEFINE_float('decay_rate', 0.97, 'decay rate for rmsprop')\n",
    "tf.flags.DEFINE_float('output_keep_prob', 1.0, 'probability of keeping weights in the hidden layer')\n",
    "tf.flags.DEFINE_float('input_keep_prob', 1.0, 'probability of keeping weights in the input layer')\n",
    "tf.flags.DEFINE_integer('vocab_size', None, 'probability of keeping weights in the input layer')\n",
    "\n",
    "# Sampling and Testing\n",
    "\n",
    "tf.flags.DEFINE_integer('n', 500, 'number of characters to sample')\n",
    "tf.flags.DEFINE_string('prime', u' ', 'prime text')\n",
    "tf.flags.DEFINE_integer('sample', 1, '0 to use max at each timestep, 1 to sample at each timestep, 2 to sample on spaces')\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "args = tf.flags.FLAGS\n",
    "\n",
    "with open(os.path.join(args.save_dir, 'chars_vocab.pkl'), 'rb') as f:\n",
    "    chars, vocab = cPickle.load(f)\n",
    "\n",
    "args.vocab_size = 65\n",
    "#Use most frequent char if no prime is given\n",
    "if args.prime == '':\n",
    "    args.prime = chars[0]\n",
    "model = Model(args, training = False)\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    ckpt = tf.train.get_checkpoint_state(args.save_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(model.sample(sess, chars, vocab, args.n, args.prime, args.sample).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
