{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy based MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, x):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "             \n",
    "    def update(self, learning_rate):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_activation(activation):\n",
    "        activations = {\n",
    "            'sigmoid': Sigmoid,\n",
    "            'relu': Relu\n",
    "        }\n",
    "        return activations[activation]\n",
    "        \n",
    "\n",
    "class Sigmoid(Activation):        \n",
    "    def forward(self, x):\n",
    "        assert x.size == self.size\n",
    "        self.y = 1 / (1 + np.exp(-x))\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, node_grad):\n",
    "        assert node_grad.size == self.size\n",
    "        return node_grad * (self.y * (1-self.y))\n",
    "    \n",
    "class Relu(Activation):        \n",
    "    def forward(self, x):\n",
    "        assert x.size == self.size\n",
    "        self.x = x\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    def backward(self, node_grad):\n",
    "        assert node_grad.size == self.size\n",
    "        return node_grad * (self.x > 0)\n",
    "    \n",
    "class Softmax_Square_Error(Activation):\n",
    "    def forward(self, x):\n",
    "        assert x.size == self.size\n",
    "        exps = np.exp(x - np.max(x)) # float64 is 10^308, exp with too large x will yield inf\n",
    "        self.y = exps / np.sum(exps) \n",
    "        return self.y\n",
    "        \n",
    "    def backward(self, label):\n",
    "        assert label.size == self.size\n",
    "        self.out_grad = (self.y - label) @ (np.diag(self.y) - np.outer(self.y, self.y))\n",
    "        return self.out_grad\n",
    "        \n",
    "class Softmax_Cross_Entropy(Activation):\n",
    "    def forward(self, x):\n",
    "        assert x.size == self.size\n",
    "        exps = np.exp(x - np.max(x)) # float64 is 10^308, exp with too large x will yield inf\n",
    "        self.y = exps / np.sum(exps)\n",
    "        return self.y\n",
    "        \n",
    "    def backward(self, label):\n",
    "        assert label.size == self.size\n",
    "        self.out_grad = self.y - label\n",
    "        return self.out_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, size_in, size_out, with_bias):\n",
    "        self.size_in = size_in\n",
    "        self.size_out = size_out\n",
    "        self.with_bias = with_bias\n",
    "        self.W = self.initialize_weight()\n",
    "        if with_bias:\n",
    "            self.b = np.zeros(size_out)\n",
    "            \n",
    "    def initialize_weight(self): # initialization is extremely important\n",
    "        #return np.zeros((self.size_in, self.size_out)) # danger if set to 0\n",
    "        #epsilon = 4.0 * np.sqrt(6) / np.sqrt(self.size_in + self.size_out) # for sigmoid\n",
    "        epsilon = np.sqrt(2.0 / (self.size_in + self.size_out)) # for relu\n",
    "        return epsilon * (np.random.rand(self.size_in, self.size_out) * 2 - 1) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert x.size == self.size_in\n",
    "        self.x = x\n",
    "        self.y = x @ self.W + self.b\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, node_grad):\n",
    "        assert node_grad.size == self.size_out\n",
    "        self.G_W = np.outer(self.x, node_grad)\n",
    "        if self.with_bias:\n",
    "            self.G_b = node_grad\n",
    "        return node_grad @ self.W.T\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        self.W -= learning_rate * self.G_W\n",
    "        if self.with_bias:\n",
    "            self.b -= learning_rate * self.G_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    def __init__(self, layer_size, with_bias=True, activation=\"sigmoid\", learning_rate=1):\n",
    "        assert len(layer_size) >= 2\n",
    "        self.layer_size = layer_size\n",
    "        self.with_bias = with_bias\n",
    "        self.activation = Activation.get_activation(activation)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        self.layers = []\n",
    "        \n",
    "        size_in = self.layer_size[0]\n",
    "        for hu in self.layer_size[1:-1]:\n",
    "            self.layers.append(Linear(size_in, hu, self.with_bias))\n",
    "            self.layers.append(self.activation(hu))\n",
    "            size_in = hu\n",
    "        self.layers.append(Linear(size_in, self.layer_size[-1], self.with_bias))\n",
    "        self.layers.append(Softmax_Cross_Entropy(self.layer_size[-1]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert x.size == self.layer_size[0]\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, label):\n",
    "        assert label.size == self.layer_size[-1]\n",
    "        node_grad = label\n",
    "        for layer in reversed(self.layers): # reversed does not make a copy\n",
    "            node_grad = layer.backward(node_grad)\n",
    "            \n",
    "    def update(self, learning_rate):\n",
    "        for layer in self.layers:\n",
    "            layer.update(learning_rate)\n",
    "            \n",
    "    def train(self, x, label):\n",
    "        assert x.size == self.layer_size[0]\n",
    "        y = self.forward(x)\n",
    "        self.backward(label)\n",
    "        self.update(self.learning_rate)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return np.argmax(x)\n",
    "    \n",
    "    def loss(self, x, label):\n",
    "        y = self.forward(x)\n",
    "        return np.sum((y - label)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "Y = np.array([\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "    [1, 0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f32df680400>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1007)\n",
    "EPOCH = 1000\n",
    "N = X.shape[0]\n",
    "\n",
    "mlp = MLP([2, 4, 2], learning_rate=.1, activation=\"relu\")\n",
    "\n",
    "loss = np.zeros(EPOCH)\n",
    "for epoch in range(EPOCH):\n",
    "    for i in range(N):\n",
    "        mlp.train(X[i], Y[i])\n",
    "        \n",
    "    #print('Prediction: ', end=\"\")\n",
    "    for i in range(N):\n",
    "        loss[epoch] += mlp.loss(X[i], Y[i])\n",
    "        #print(mlp.predict(X[i]), end=\" \")\n",
    "        \n",
    "    loss[epoch] /= N\n",
    "    #print('Epoch %d, Loss: %f' % (epoch, loss[epoch]))\n",
    "    \n",
    "plt.figure()\n",
    "ix = np.arange(EPOCH)\n",
    "plt.plot(ix, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7532d6a377b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(2, input_dim=2, activation=tf.nn.sigmoid))\n",
    "model.add(Dense(2, activation=tf.nn.softmax))\n",
    "\n",
    "sgd = SGD(lr=1)\n",
    "\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X, Y, epochs=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1\n",
    "training_epochs = 200\n",
    "\n",
    "d_input = 2\n",
    "d_hidden = 10\n",
    "d_output = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None, d_input])\n",
    "y = tf.placeholder(\"float\", [None, d_output])\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([d_input, d_hidden]))\n",
    "b1 = tf.Variable(tf.random_normal([d_hidden]))\n",
    "h1 = tf.nn.relu(tf.add(tf.matmul(x, w1), b1))\n",
    "\n",
    "w2 = tf.Variable(tf.random_normal([d_hidden, d_output]))\n",
    "b2 = tf.Variable(tf.random_normal([d_output]))\n",
    "out = tf.add(tf.matmul(h1, w2), b2)\n",
    "\n",
    "pred = tf.nn.softmax(out)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: X, y: Y})\n",
    "        print(\"Epoch: %d, cost=%.9f\" % (epoch+1, c))\n",
    "        \n",
    "    y_pred = sess.run([pred], feed_dict={x: X})\n",
    "    print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
