{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "- Implement Random Forest using sklearn.tree.DecisionTreeClassifier\n",
    "- Out-of-Bag Error\n",
    "- Learn bagging and boosting in sklearn\n",
    "- visualize bias-variance for bagging and boosting\n",
    "- stacking\n",
    "- play on California Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=2000, \n",
    "                           n_features=16, n_informative=6,\n",
    "                           n_clusters_per_class=2, flip_y=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Build random forest based on sklearn.tree.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "\n",
    "class RandomForest():\n",
    "    def __init__(self, n_trees=10, max_features='sqrt', oob_score=False):\n",
    "        self.n_trees = n_trees\n",
    "        self.oob_score = oob_score\n",
    "        self.trees = [DTC(max_features=max_features) for _ in range(n_trees)]\n",
    "\n",
    "    # compatible with Sklearn fit api, train model with X and y\n",
    "    # X: training data of shape [n_sample, d_feature]\n",
    "    # y: class label of shape [n_sample]\n",
    "    def fit(self, X, y):\n",
    "        # TODO\n",
    "    \n",
    "    # compatible with Sklearn predict api, get class prediction with X\n",
    "    # X: data of shape [n_sample, d_feature]\n",
    "    # return: prediction of shape [n_sample]\n",
    "    def predict(self, X):\n",
    "        # TODO\n",
    "    \n",
    "    # compatible with Sklearn predict_proba api, get class predict probability with X\n",
    "    # X: data of shape [n_sample, d_feature]\n",
    "    # return: predict probability of shape [n_sample, n_class]\n",
    "    def predict_proba(self, X):\n",
    "        # TODO\n",
    "    \n",
    "    # compatible with Sklearn score api, compute classification accuracy using trained model\n",
    "    # X: data of shape [n_sample, d_feature]\n",
    "    # y: class label of shape [n_sample]\n",
    "    # return: accuracy\n",
    "    def score(self, X, y):\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bc = BaggingClassifier(DTC(max_features='sqrt'), n_estimators=10, oob_score=True)\n",
    "bc.fit(X, y)\n",
    "print(bc.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualization for Bagging (reduce variance)\n",
    "n_trees = []\n",
    "oob_score = []\n",
    "train_score = []\n",
    "for n_tree in range(1, 50):\n",
    "    rf = RandomForest(n_trees = n_tree, oob_score=True)\n",
    "    rf.fit(X, y)\n",
    "    n_trees.append(n_tree)\n",
    "    oob_score.append(rf.oob_score_)\n",
    "    train_score.append(rf.score(X, y))\n",
    "    \n",
    "plt.plot(n_trees, oob_score, label='oob_score')\n",
    "plt.plot(n_trees, train_score, label='train_score')\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('n_trees')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization for Boosting (reduce bias)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "n_trees = []\n",
    "train_score = []\n",
    "test_score = []\n",
    "\n",
    "for n_estimators in range(1, 100, 1):\n",
    "    gbc = GBC(n_estimators=n_estimators, max_depth=3)\n",
    "    gbc.fit(X_train, y_train)\n",
    "    n_trees.append(n_estimators)\n",
    "    train_score.append(gbc.score(X_train, y_train))\n",
    "    test_score.append(gbc.score(X_test, y_test))\n",
    "    \n",
    "plt.plot(n_trees, train_score, label='train_score')\n",
    "plt.plot(n_trees, test_score, label='test_score')\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('n_trees')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Ensemble Classifier\n",
    "- **AverageClassifier**: average the predict probability of base classifiers\n",
    "- **StackingClassifier**: use the predict probability of base classifiers (option: combined with original features) as new features then train a meta classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class EnsembleClassifier():\n",
    "    # classifiers: list of base classifiers (with method fit, score, predict_proba)\n",
    "    def __init__(self, classifiers):\n",
    "        self.classifiers = classifiers\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # get score of each base classifier\n",
    "    # X: data of shape [n_sample, d_feature]\n",
    "    # y: class label of shape [n_sample]\n",
    "    # return: list of scores\n",
    "    def score_classifiers(self, X, y):\n",
    "        # TODO\n",
    "        \n",
    "class AverageClassifier(EnsembleClassifier):\n",
    "    def fit(self, X, y):\n",
    "        # TODO\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # TODO\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        # TODO\n",
    "    \n",
    "class StackingClassifier(EnsembleClassifier):\n",
    "    # meta_classifier: classifier for second-level prediction\n",
    "    # concat_feature: whether to use original feature\n",
    "    # kfold: split training data into kfold, train on k-1 folds, get prediction on the rest one\n",
    "    def __init__(self, classifiers, meta_classifier, concat_feature=False, kfold=5):\n",
    "        # TODO\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # TODO\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # TODO\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNC\n",
    "\n",
    "rf = RFC(n_estimators=10)\n",
    "knc = KNC()\n",
    "lr = LR(C=2, solver='liblinear', multi_class=\"ovr\")\n",
    "\n",
    "ac = AverageClassifier([rf, knc])\n",
    "ac.fit(X_train, y_train)\n",
    "print(\"BaseClassifiers: \", ac.score_classifiers(X_test, y_test))\n",
    "print(\"AverageClassifiers: %.6f\" % ac.score(X_test, y_test))\n",
    "\n",
    "sc = StackingClassifier([rf, knc], lr, concat_feature=False)\n",
    "sc.fit(X_train, y_train)\n",
    "print(\"StackingClassifiers: %.6f\" % sc.score(X_test, y_test))\n",
    "\n",
    "sc_concat = StackingClassifier([rf, knc], lr, concat_feature=True)\n",
    "sc_concat.fit(X_train, y_train)\n",
    "print(\"StackingClassifiers with original features: %.6f\" % sc_concat.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare California Housing Dataset\n",
    "This dataset consists of 20,640 samples and 8 features.\n",
    "The original target is real number, transform it classes first for demo purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "cal_housing = datasets.fetch_california_housing()\n",
    "X, y_real = cal_housing.data, cal_housing.target\n",
    "y = np.zeros(y_real.shape[0])\n",
    "\n",
    "plt.hist(y_real, bins=50)\n",
    "plt.show()\n",
    "\n",
    "# skewed number of samples with value > 4.9 according to the histogram\n",
    "y[np.where(y_real > 4.9)] = 4\n",
    "# divide rest of samples into 4 bins\n",
    "# please check np.digitize, np.histogram on numpy document\n",
    "# Notice: if you are using sklearn 0.20.1, there is a handy helper sklearn.preprocessing.KBinsDiscretizer\n",
    "y_normal_idx = np.where(y_real <= 4.9)\n",
    "y_normal = y_real[y_normal_idx]\n",
    "y[y_normal_idx] = np.digitize(y_normal, bins=np.histogram(y_normal, bins=4)[1][1:], right=True)\n",
    "y.astype(np.int32, copy=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
